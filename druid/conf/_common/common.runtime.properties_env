#
# Extensions
#
druid.extensions.loadList=["druid-hdfs-storage", "druid-kafka-indexing-service", "druid-datasketches", "druid-basic-security", "druid-histogram", "druid-lookups-cached-global", "postgresql-metadata-storage", "druid-multi-stage-query", "druid-avro-extensions"]

# If you have a different version of Hadoop, place your Hadoop client jar files in your hadoop-dependencies directory
# and uncomment the line below to point to your directory.
#druid.extensions.hadoopDependenciesDir=/my/dir/hadoop-dependencies

#
# Logging
#

# Log all runtime properties on startup. Disable to avoid logging properties on startup:
druid.emitter=logging
druid.emitter.logging.logLevel=${LOG_LEVEL}
druid.startup.logging.logProperties=true

#
# Zookeeper
#
druid.zk.service.host=${ZOOKEEPER_SERVER}
druid.zk.paths.base=/druid

#
# Metadata storage
#

druid.metadata.storage.type=postgresql
druid.metadata.storage.connector.connectURI=jdbc:postgresql://${METADATA_STORAGE_HOST}:${METADATA_STORAGE_PORT}/druid
druid.metadata.storage.connector.user=${METADATA_STORAGE_USER}
druid.metadata.storage.connector.password=${METADATA_STORAGE_PASSWORD}

#
# Deep storage
#

# For local disk (only viable in a cluster if this is a network mount):
# druid.storage.type=local
# druid.storage.storageDirectory=var/druid/segments

druid.storage.type=hdfs
#druid.storage.storageDirectory=hdfs://${DEEP_STORAGE_HOST}:${DEEP_STORAGE_PORT}/druid/segments
druid.storage.storageDirectory=druid/segments

#
# Indexing service logs
#

# For local disk (only viable in a cluster if this is a network mount):
druid.indexer.logs.type=hdfs
druid.indexer.logs.directory=druid/indexing-logs
druid.indexer.logs.kill.enabled=true
druid.indexer.logs.kill.durationToRetain=43200000

#
# Service discovery
#
druid.selectors.indexing.serviceName=druid/overlord
druid.selectors.coordinator.serviceName=druid/coordinator

#
# Monitoring
#
druid.monitoring.monitors=["io.druid.java.util.metrics.JvmMonitor"]
druid.emitter=logging
druid.emitter.logging.logLevel=DEBUG

# Storage type of double columns
# ommiting this will lead to index double as float at the storage layer
druid.indexing.doubleStorage=double

# SQL
druid.sql.enable=true

# Druid basic security
druid.auth.authenticatorChain=["MyBasicMetadataAuthenticator"]
druid.auth.authenticator.MyBasicMetadataAuthenticator.type=basic

# Default password for 'admin' user, should be changed for production.
druid.auth.authenticator.MyBasicMetadataAuthenticator.initialAdminPassword=druidAdminPassword

# Default password for internal 'druid_system' user, should be changed for production.
druid.auth.authenticator.MyBasicMetadataAuthenticator.initialInternalClientPassword=druidClientPassword

# Uses the metadata store for storing users, you can use authentication API to create new users and grant permissions
druid.auth.authenticator.MyBasicMetadataAuthenticator.credentialsValidator.type=metadata

# If true and the request credential doesn't exists in this credentials store, the request will proceed to next Authenticator in the chain.
druid.auth.authenticator.MyBasicMetadataAuthenticator.skipOnFailure=false

druid.auth.authenticator.MyBasicMetadataAuthenticator.authorizerName=MyBasicMetadataAuthorizer

# Escalator
druid.escalator.type=basic
druid.escalator.internalClientUsername=druid_system
druid.escalator.internalClientPassword=druidClientPassword
druid.escalator.authorizerName=MyBasicMetadataAuthorizer

druid.auth.authorizers=["MyBasicMetadataAuthorizer"]

druid.auth.authorizer.MyBasicMetadataAuthorizer.type=basic
